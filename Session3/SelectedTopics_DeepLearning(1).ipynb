{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SelectedTopics_DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdOBWRwDPcfw"
      },
      "source": [
        "# Imports and definition of helper functions\n",
        "The first block imports all necessary packages, this should normally not be changed unless you want to build custom functions or networks.\n",
        "\n",
        "The second block defines some helper functions. You might want to have a look here how they look, but everything should be fine as is.\n",
        "\n",
        "**Don't forget to enable the GPU for your notebook if you want hardware acceleration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRWMhkulOxHI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "from sklearn import metrics\n",
        "from scipy import io as sio\n",
        "from scipy import signal as sg\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Reshape, MaxPool1D, Activation, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMmARoeiODnM"
      },
      "source": [
        " def plot_performance_curves(y_true, y_proba, save_path=None):\n",
        "  \"\"\"\n",
        "  Plot ROC-curve and PR-curve\n",
        "  \"\"\"\n",
        "  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
        "\n",
        "  # ROC curve\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_true=y_true, y_score=y_proba)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "  lw = 2\n",
        "  ax1.plot(fpr, tpr, color='C0', lw=lw,\n",
        "            label='AUC = {0:.3f}'.format(roc_auc))\n",
        "  ax1.plot([0, 1], [0, 1], color='C1', lw=lw, linestyle='--')\n",
        "  ax1.set_xlim([0., 1.])\n",
        "  ax1.set_ylim([0., 1.05])\n",
        "  ax1.set_xlabel('1 - Specificity')\n",
        "  ax1.set_ylabel('Sensitivity')\n",
        "  ax1.set_title('Receiver Operating Characteristic')\n",
        "  ax1.legend(loc='lower right')\n",
        "  sns.despine(trim=True, ax=ax1)\n",
        "\n",
        "  # PR curve\n",
        "  prec, rec, _ = metrics.precision_recall_curve(y_true=y_true, probas_pred=y_proba)\n",
        "  ap = metrics.average_precision_score(y_true=y_true, y_score=y_proba)\n",
        "\n",
        "  ax2.plot(rec, prec, drawstyle='steps-post', label='AP = {0:.3f}'.format(ap))\n",
        "  ax2.set_xlim([0., 1.])\n",
        "  ax2.set_ylim([0., 1.])\n",
        "  ax2.set_xlabel('Recall')\n",
        "  ax2.set_ylabel('Precision')\n",
        "  ax2.set_title('Precision-recall Curve')\n",
        "  ax2.legend()\n",
        "  sns.despine(trim=True, ax=ax2)\n",
        "\n",
        "  if save_path:\n",
        "    plt.savefig(save_path+'.pdf', bbox_inches='tight')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6dQEfgqPVEn"
      },
      "source": [
        "# Upload data\n",
        "Run this section to upload the `deeplearning_data.zip` file from Toledo. This will likely take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt9wZHjZOpvb"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rsk_bc3NMxf"
      },
      "source": [
        "!unzip deeplearning_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqs6oMB0OfpR"
      },
      "source": [
        "# Data loading and normalization\n",
        "Here we go from the `npy` files to usable Numpy arrays. The first block loads all necessary arrays into memory.\n",
        "\n",
        "The second one performs data normalization by default. You can disable data normalization by running this section again and setting the `normalize` variable to `False`. Every recording is individually rescaled to unit variance. Feel free to change to a different normalization scheme but document this clearly in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFXsRHs-O8tc"
      },
      "source": [
        "x_train = np.load('x_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "\n",
        "x_test = np.load('x_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "classical_features = np.load('classical_features.npy')\n",
        "classical_labels = np.load('classical_labels.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAdJxpO6QAlV"
      },
      "source": [
        "normalize = True\n",
        "\n",
        "if normalize:\n",
        "  train_mean = np.mean(x_train, axis=-1, keepdims=True)\n",
        "  train_std = np.std(x_train, axis=-1, keepdims=True)\n",
        "  test_mean = np.mean(x_test, axis=-1, keepdims=True)\n",
        "  test_std = np.std(x_test, axis=-1, keepdims=True)\n",
        "\n",
        "  x_train = (x_train - train_mean) / train_std\n",
        "  x_test = (x_test - test_mean) / test_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rbxb8uKQ_U5"
      },
      "source": [
        "# Data 'exploration'\n",
        "Some example plots + label distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir8SdNC3SXrJ"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,4))\n",
        "\n",
        "index_normal = np.where(y_train==0)[0][0]\n",
        "index_af = np.where(y_train==1)[0][2]\n",
        "\n",
        "ax1.plot(np.linspace(0, 30, 6000), x_train[index_normal])\n",
        "sns.despine(trim=True, ax=ax1)\n",
        "ax1.set_title('Example of a normal rhythm')\n",
        "ax1.set_xlabel('Time (s)')\n",
        "ax1.set_ylabel('Amplitude')\n",
        "\n",
        "ax2.plot(np.linspace(0, 30, 6000), x_train[index_af])\n",
        "sns.despine(trim=True, ax=ax2)\n",
        "ax2.set_title('Example of fibrillation')\n",
        "ax2.set_xlabel('Time (s)')\n",
        "ax2.set_ylabel('Amplitude')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epXyPin0RHwp"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "\n",
        "ax1.bar(x=[0, 1], height=[np.sum(y_train==0), np.sum(y_train==1)], width=0.6)\n",
        "ax1.set_xticks([0,1])\n",
        "ax1.set_xticklabels(['Normal', 'AF'], size=15)\n",
        "ax1.set_title('Training Label Distribution', size=12)\n",
        "\n",
        "ax2.bar(x=[0, 1], height=[np.sum(y_test==0), np.sum(y_test==1)], width=0.6)\n",
        "ax2.set_xticks([0,1])\n",
        "ax2.set_xticklabels(['Normal', 'AF'], size=15)\n",
        "ax2.set_title('Validation Label Distribution', size=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKkegHNPLZ9M"
      },
      "source": [
        "# Constructing the neural network\n",
        "Use this section to build your own neural network. You can follow any basic Keras tutorial, just remember we are working with time series instead of images. (So, use `Conv1D` instead of `Conv2D`)\n",
        "\n",
        "The function below assumes use of the `Functional` API. Many Keras tutorials start out with its `Sequenctial` API, but this way of building networks can easily become too limiting once you move away from simple networks.\n",
        "\n",
        "The main way of building a network in the `Functional` API is \"chaining\" together different layers. In Python code, this follows the template of \n",
        "```\n",
        "new_layer = <Layer_Type>(layer_parameters)(previous_layer)\n",
        "```\n",
        "You can chain various layer types together this way, and branch out or combine them. The default building block for most CNNs these days is to use a chain of convolution (without activation) -> batch normalization -> ReLU (-> pooling). You can start out with a few of these blocks, or try something else."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn():\n",
        " input_length = 6000\n",
        " input_signal = Input(shape=(input_length,))\n",
        " x = Reshape(target_shape=(input_length, 1))(input_signal)\n",
        "\n",
        " #################################################\n",
        " # BUILD YOUR NETWORK HERE\n",
        " # You can rely on the `GlobalAveragePooling1D`\n",
        " # below, or implement your own pooling operation\n",
        " # to go to a feature representation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " x = GlobalAveragePooling1D()(x)\n",
        " #################################################\n",
        "  \n",
        " \n",
        "\n",
        " output = Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        " clf = Model(input_signal, output)\n",
        " feature_extraction = Model(input_signal, x)\n",
        "\n",
        " return clf, feature_extraction"
      ],
      "metadata": {
        "id": "3fMn0V21ME8D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SexzIfUxQdsT"
      },
      "source": [
        "clf, feature_extraction = build_cnn()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdUT1-PdQvW5"
      },
      "source": [
        "clf.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHh8h-6Rs7X"
      },
      "source": [
        "# Training the network\n",
        "Some parameters to influence the training process. Calling `clf.fit()` starts the training process and outputs a `History` object containing variables that Keras tracks during training.\n",
        "\n",
        "- `n_epochs`\n",
        "- `batch_size`\n",
        "- `lr_init` initial learning rate\n",
        "- `opt` is an instance of a Keras optimizer. `tf.keras.optimizers` contains other optimizers you can try.\n",
        "- `loss` By default the cross-entropy loss. You probably do not want to change this.\n",
        "- `metrics` A python list of strings, containing metrics specified in `tf.keras.metrics`. Think about what other metrics might be interesting to track during training. Keep in mind that this an unbalanced data set.\n",
        "- `use_decay`, A boolean telling whether or not to use learning rate decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGnbFplRwSi"
      },
      "source": [
        "n_epochs = 200\n",
        "batch_size = 128\n",
        "lr_init = 1e-3\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=lr_init)\n",
        "\n",
        "loss = 'binary_crossentropy'\n",
        "training_metrics = ['accuracy']\n",
        "\n",
        "use_decay = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHFq6C6Xc2ab"
      },
      "source": [
        "def scheduler_function(epoch, lr):\n",
        "  return np.float32(lr_init * math.pow(0.5, epoch//20))\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22K-a5wjc_R5"
      },
      "source": [
        "clf.compile(optimizer=opt, loss=loss, metrics=training_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Rg-9POKe1uA"
      },
      "source": [
        "if use_decay:\n",
        "  history = clf.fit(x=x_train, y=y_train,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    batch_size=batch_size, epochs=n_epochs,\n",
        "                    callbacks=[scheduler])\n",
        "else:\n",
        "  history = clf.fit(x=x_train, y=y_train,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    batch_size=batch_size, epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL0cw05QzQVe"
      },
      "source": [
        "# Assessing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SlLbWhXzYAH"
      },
      "source": [
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBpJN1b0ztmj"
      },
      "source": [
        "plt.plot(training_loss, label='Training')\n",
        "plt.plot(validation_loss, label='Validation')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross-entropy loss')\n",
        "plt.legend()\n",
        "sns.despine(trim=True)\n",
        "plt.savefig('loss.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qEiYUy6glu"
      },
      "source": [
        "y_proba = clf.predict(x_test)\n",
        "y_pred = y_proba > 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flSf5EqGKB3D"
      },
      "source": [
        "acc = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "f1 = metrics.f1_score(y_true=y_test, y_pred=y_pred)\n",
        "print('Accuracy:    {0:.3f}'.format(acc))\n",
        "print('F1-score:    {0:.3f}'.format(f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vFR6EQ-6rKj"
      },
      "source": [
        "plot_performance_curves(y_true=y_test, y_proba=y_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ou99MNM0wCa"
      },
      "source": [
        "# Investigating the feature space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnMFY70T8l-X"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "tsne = TSNE(n_components=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ud_WoodDm9Y"
      },
      "source": [
        "We give you the feature space plots of classical features as example. The classical feature arrays also contain entries for the other classes in the dataset, these should be filtered out first. The first two 'features' in the `classical_features` array do not contain meaningful features but are leftovers from the feature extraction process. They should be ignored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc-skcwQ_6vS"
      },
      "source": [
        "idx = np.logical_or(classical_labels==0, classical_labels==1)\n",
        "\n",
        "classical_pca = pca.fit_transform(classical_features[idx, 2:])\n",
        "classical_tsne = tsne.fit_transform(classical_features[idx, 2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwQpcJoM_7Or"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
        "\n",
        "normal_idx = classical_labels[idx] == 0\n",
        "af_idx = classical_labels[idx] == 1\n",
        "\n",
        "ax1.scatter(classical_pca[normal_idx, 0], classical_pca[normal_idx, 1], label='Normal', s=5, alpha=0.5)\n",
        "ax1.scatter(classical_pca[af_idx, 0], classical_pca[af_idx, 1], label='AF', s=5, alpha=0.5)\n",
        "ax1.legend()\n",
        "sns.despine(trim=True, ax=ax1)\n",
        "ax1.set_title('PCA visualization')\n",
        "\n",
        "ax2.scatter(classical_tsne[normal_idx, 0], classical_tsne[normal_idx, 1], label='Normal', s=5, alpha=0.5)\n",
        "ax2.scatter(classical_tsne[af_idx, 0], classical_tsne[af_idx, 1], s=5, alpha=0.5)\n",
        "ax2.legend()\n",
        "sns.despine(trim=True, ax=ax2)\n",
        "ax2.set_title('t-SNE visualization')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magI0AUZYcuK"
      },
      "source": [
        "Try and visualize the feature space of your final model in a similar way. The `feature_extraction` model gives you the activations of your complete classifier before the final dense layer using the learned network weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TICAie0R96qt"
      },
      "source": [
        "f = feature_extraction.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heqDtTxT97n5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LoUcY5q97vQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}